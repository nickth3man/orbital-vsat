# Data Management Strategy

## Overview

This phase focuses on implementing an effective strategy for managing the audio files, processing results, and analysis data generated by VSAT. As the application's sole user, you need a personalized data management approach that balances storage efficiency with quick access to your processed audio and analysis results.

With a proper data management strategy, you'll be able to easily locate past work, minimize duplicate processing efforts, and ensure that valuable derived data is preserved appropriately while temporary files are systematically pruned. This guide will help you implement file organization, metadata tagging, search capabilities, and automated data maintenance policies.

## Prerequisites

Before implementing your data management strategy, ensure you have:

- [ ] Completed documentation of your workflows
- [ ] Decided on your primary storage locations
- [ ] Identified typical data outputs from your workflows
- [ ] 4-6 hours of implementation time
- [ ] Backup of any existing data you'll be reorganizing

## Designing a File Organization System

### 1. Create a Storage Manager

Implement a system to manage file storage locations and policies:

```python
# data/storage_manager.py

class StorageManager:
    def __init__(self, config_path="./config/storage.json"):
        self.config_path = config_path
        self.config = self._load_config()
        self._ensure_directories_exist()
        
    def _load_config(self):
        """Load storage configuration"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r') as f:
                return json.load(f)
                
        # Create default configuration
        default_config = self._create_default_config()
        self._save_config(default_config)
        return default_config
        
    def _create_default_config(self):
        """Create default storage configuration"""
        return {
            'storage_locations': {
                'projects': './data/projects',
                'exports': './data/exports',
                'cache': './data/cache',
                'temp': './data/temp',
                'analysis': './data/analysis',
                'backups': './data/backups'
            },
            'retention_policies': {
                'cache': {
                    'max_size_gb': 10,
                    'max_age_days': 30
                },
                'temp': {
                    'max_size_gb': 5,
                    'max_age_days': 7
                },
                'backups': {
                    'max_versions': 5,
                    'max_age_days': 90
                }
            },
            'file_types': {
                'source_audio': ['.wav', '.mp3', '.flac', '.ogg', '.m4a'],
                'processed_audio': ['.wav'],
                'analysis_data': ['.json', '.npy', '.pkl'],
                'project_files': ['.vsat']
            },
            'metadata_fields': [
                'title',
                'artist',
                'album',
                'genre',
                'processing_date',
                'model_used',
                'voice_count',
                'processing_quality',
                'duration',
                'sample_rate',
                'channels',
                'bit_depth',
                'tags'
            ]
        }
        
    def _save_config(self, config):
        """Save storage configuration"""
        os.makedirs(os.path.dirname(self.config_path), exist_ok=True)
        with open(self.config_path, 'w') as f:
            json.dump(config, f, indent=2)
            
    def _ensure_directories_exist(self):
        """Create storage directories if they don't exist"""
        for location, path in self.config['storage_locations'].items():
            os.makedirs(path, exist_ok=True)
            
    def get_path(self, location_type):
        """Get path for a specific storage location"""
        if location_type not in self.config['storage_locations']:
            raise ValueError(f"Unknown storage location: {location_type}")
            
        return self.config['storage_locations'][location_type]
        
    def get_project_path(self, project_name):
        """Get path for a specific project"""
        projects_dir = self.get_path('projects')
        return os.path.join(projects_dir, self.sanitize_filename(project_name))
        
    def ensure_project_directory(self, project_name):
        """Create project directory if it doesn't exist"""
        project_path = self.get_project_path(project_name)
        if not os.path.exists(project_path):
            # Create project directory with subdirectories
            os.makedirs(project_path, exist_ok=True)
            os.makedirs(os.path.join(project_path, 'source'), exist_ok=True)
            os.makedirs(os.path.join(project_path, 'separated'), exist_ok=True)
            os.makedirs(os.path.join(project_path, 'analysis'), exist_ok=True)
            
        return project_path
        
    def sanitize_filename(self, filename):
        """Convert a string to a valid filename"""
        # Replace spaces with underscores and remove invalid characters
        valid_filename = re.sub(r'[^\w\-\.]', '_', filename.replace(' ', '_'))
        return valid_filename
        
    def store_file(self, file_path, location_type, subdirectory=None, rename=None):
        """Store a file in the specified location"""
        if location_type not in self.config['storage_locations']:
            raise ValueError(f"Unknown storage location: {location_type}")
            
        dest_dir = self.config['storage_locations'][location_type]
        
        if subdirectory:
            dest_dir = os.path.join(dest_dir, subdirectory)
            os.makedirs(dest_dir, exist_ok=True)
            
        filename = os.path.basename(file_path)
        if rename:
            filename = rename
            
        dest_path = os.path.join(dest_dir, filename)
        
        # Copy file
        shutil.copy2(file_path, dest_path)
        
        return dest_path
        
    def apply_retention_policy(self, location_type):
        """Apply retention policy to a storage location"""
        if location_type not in self.config['retention_policies']:
            return False
            
        policy = self.config['retention_policies'][location_type]
        location_path = self.config['storage_locations'][location_type]
        
        if not os.path.exists(location_path):
            return False
            
        # Apply age-based policy
        if 'max_age_days' in policy:
            max_age = policy['max_age_days']
            cutoff_time = time.time() - (max_age * 86400)  # Convert days to seconds
            
            for root, dirs, files in os.walk(location_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if os.path.getmtime(file_path) < cutoff_time:
                        try:
                            os.remove(file_path)
                        except OSError:
                            pass
                            
        # Apply size-based policy
        if 'max_size_gb' in policy:
            max_size_bytes = policy['max_size_gb'] * 1024 * 1024 * 1024
            
            # Get all files with their modification times
            all_files = []
            for root, dirs, files in os.walk(location_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    all_files.append((file_path, os.path.getmtime(file_path), os.path.getsize(file_path)))
                    
            # Sort by modification time (oldest first)
            all_files.sort(key=lambda x: x[1])
            
            # Calculate current size
            current_size = sum(file[2] for file in all_files)
            
            # Remove oldest files until under the size limit
            for file_path, mtime, size in all_files:
                if current_size <= max_size_bytes:
                    break
                    
                try:
                    os.remove(file_path)
                    current_size -= size
                except OSError:
                    pass
                    
        return True
```

### 2. Implement a Project Manager

Create a system to manage projects and their associated files:

```python
# data/project_manager.py

class ProjectManager:
    def __init__(self):
        self.storage_manager = StorageManager()
        
    def create_project(self, project_name, metadata=None):
        """Create a new project"""
        # Create project directory
        project_path = self.storage_manager.ensure_project_directory(project_name)
        
        # Create project metadata file
        metadata = metadata or {}
        metadata.update({
            'project_name': project_name,
            'created_date': datetime.now().isoformat(),
            'modified_date': datetime.now().isoformat(),
            'files': []
        })
        
        metadata_path = os.path.join(project_path, 'project.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
            
        return project_path
        
    def get_project_metadata(self, project_name):
        """Get project metadata"""
        project_path = self.storage_manager.get_project_path(project_name)
        metadata_path = os.path.join(project_path, 'project.json')
        
        if not os.path.exists(metadata_path):
            return None
            
        with open(metadata_path, 'r') as f:
            return json.load(f)
            
    def update_project_metadata(self, project_name, metadata_updates):
        """Update project metadata"""
        metadata = self.get_project_metadata(project_name)
        
        if not metadata:
            return False
            
        # Update metadata
        metadata.update(metadata_updates)
        metadata['modified_date'] = datetime.now().isoformat()
        
        # Save updated metadata
        project_path = self.storage_manager.get_project_path(project_name)
        metadata_path = os.path.join(project_path, 'project.json')
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
            
        return True
        
    def add_file_to_project(self, project_name, file_path, file_type, metadata=None):
        """Add a file to a project"""
        project_path = self.storage_manager.get_project_path(project_name)
        
        if not os.path.exists(project_path):
            return False
            
        # Determine appropriate subdirectory based on file type
        if file_type == 'source':
            subdir = 'source'
        elif file_type == 'separated':
            subdir = 'separated'
        elif file_type == 'analysis':
            subdir = 'analysis'
        else:
            subdir = ''
            
        # Copy file to project directory
        dest_dir = os.path.join(project_path, subdir)
        os.makedirs(dest_dir, exist_ok=True)
        
        filename = os.path.basename(file_path)
        dest_path = os.path.join(dest_dir, filename)
        
        shutil.copy2(file_path, dest_path)
        
        # Update project metadata
        file_metadata = metadata or {}
        file_metadata.update({
            'filename': filename,
            'original_path': file_path,
            'project_path': os.path.relpath(dest_path, project_path),
            'file_type': file_type,
            'added_date': datetime.now().isoformat()
        })
        
        # Add file info to project metadata
        project_metadata = self.get_project_metadata(project_name)
        
        if not project_metadata:
            return False
            
        project_metadata['files'].append(file_metadata)
        project_metadata['modified_date'] = datetime.now().isoformat()
        
        # Save updated metadata
        metadata_path = os.path.join(project_path, 'project.json')
        with open(metadata_path, 'w') as f:
            json.dump(project_metadata, f, indent=2)
            
        return dest_path
        
    def list_projects(self):
        """List all projects"""
        projects_dir = self.storage_manager.get_path('projects')
        
        if not os.path.exists(projects_dir):
            return []
            
        projects = []
        
        for item in os.listdir(projects_dir):
            project_path = os.path.join(projects_dir, item)
            
            if os.path.isdir(project_path):
                # Check if it has a project.json file
                metadata_path = os.path.join(project_path, 'project.json')
                
                if os.path.exists(metadata_path):
                    try:
                        with open(metadata_path, 'r') as f:
                            metadata = json.load(f)
                            
                        projects.append({
                            'name': metadata.get('project_name', item),
                            'path': project_path,
                            'created': metadata.get('created_date'),
                            'modified': metadata.get('modified_date'),
                            'file_count': len(metadata.get('files', []))
                        })
                    except (json.JSONDecodeError, IOError):
                        # Add with minimal info if metadata file is corrupted
                        projects.append({
                            'name': item,
                            'path': project_path,
                            'created': None,
                            'modified': None,
                            'file_count': 0
                        })
                        
        return projects
```

## Implementing Metadata Management

### 1. Create a Metadata Extractor

Implement a system to extract metadata from audio files:

```python
# data/metadata_extractor.py

class MetadataExtractor:
    def __init__(self):
        pass
        
    def extract_audio_metadata(self, file_path):
        """Extract metadata from an audio file"""
        if not os.path.exists(file_path):
            return None
            
        try:
            # Use mutagen for audio metadata
            audio = mutagen.File(file_path)
            
            if audio is None:
                return self._extract_basic_info(file_path)
                
            # Extract common metadata
            metadata = {
                'filename': os.path.basename(file_path),
                'file_path': file_path,
                'file_size': os.path.getsize(file_path),
                'format': audio.mime[0].split('/')[-1] if hasattr(audio, 'mime') and audio.mime else None
            }
            
            # Extract audio-specific info
            if hasattr(audio, 'info'):
                metadata.update({
                    'duration': audio.info.length,
                    'sample_rate': getattr(audio.info, 'sample_rate', None),
                    'channels': getattr(audio.info, 'channels', None),
                    'bit_depth': getattr(audio.info, 'bits_per_sample', None),
                    'bitrate': getattr(audio.info, 'bitrate', None)
                })
                
            # Extract tags
            if hasattr(audio, 'tags'):
                tag_mapping = {
                    'title': ['title', 'TIT2'],
                    'artist': ['artist', 'TPE1'],
                    'album': ['album', 'TALB'],
                    'genre': ['genre', 'TCON'],
                    'date': ['date', 'TDRC'],
                    'track': ['tracknumber', 'TRCK'],
                    'comment': ['comment', 'COMM']
                }
                
                for key, tag_keys in tag_mapping.items():
                    for tag_key in tag_keys:
                        if tag_key in audio.tags:
                            metadata[key] = str(audio.tags[tag_key])
                            break
                            
            return metadata
            
        except Exception as e:
            # Fallback to basic file info
            logging.warning(f"Failed to extract metadata from {file_path}: {e}")
            return self._extract_basic_info(file_path)
            
    def _extract_basic_info(self, file_path):
        """Extract basic file information"""
        return {
            'filename': os.path.basename(file_path),
            'file_path': file_path,
            'file_size': os.path.getsize(file_path),
            'format': os.path.splitext(file_path)[1][1:],
            'created': datetime.fromtimestamp(os.path.getctime(file_path)).isoformat(),
            'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
        }
        
    def extract_processing_metadata(self, original_file, process_type, parameters=None):
        """Create metadata for processed files"""
        base_metadata = self.extract_audio_metadata(original_file) if os.path.exists(original_file) else {}
        
        processing_metadata = {
            'original_file': os.path.basename(original_file),
            'process_type': process_type,
            'processing_date': datetime.now().isoformat(),
            'parameters': parameters or {}
        }
        
        # Combine metadata
        combined = {**base_metadata, **processing_metadata}
        return combined
```

### 2. Implement a Metadata Database

Create a database to store and query metadata:

```python
# data/metadata_db.py

class MetadataDatabase:
    def __init__(self, db_path="./data/metadata.db"):
        self.db_path = db_path
        self._ensure_db_exists()
        
    def _ensure_db_exists(self):
        """Create the database if it doesn't exist"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create tables
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY,
                filename TEXT,
                file_path TEXT UNIQUE,
                file_type TEXT,
                file_size INTEGER,
                format TEXT,
                created_date TEXT,
                modified_date TEXT,
                indexed_date TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS audio_metadata (
                file_id INTEGER PRIMARY KEY,
                duration REAL,
                sample_rate INTEGER,
                channels INTEGER,
                bit_depth INTEGER,
                bitrate INTEGER,
                title TEXT,
                artist TEXT,
                album TEXT,
                genre TEXT,
                date TEXT,
                track TEXT,
                comment TEXT,
                FOREIGN KEY (file_id) REFERENCES files (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processing_metadata (
                id INTEGER PRIMARY KEY,
                file_id INTEGER,
                original_file_id INTEGER,
                process_type TEXT,
                processing_date TEXT,
                parameters TEXT,
                FOREIGN KEY (file_id) REFERENCES files (id),
                FOREIGN KEY (original_file_id) REFERENCES files (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS projects (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE,
                path TEXT,
                created_date TEXT,
                modified_date TEXT,
                description TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS project_files (
                project_id INTEGER,
                file_id INTEGER,
                file_type TEXT,
                added_date TEXT,
                PRIMARY KEY (project_id, file_id),
                FOREIGN KEY (project_id) REFERENCES projects (id),
                FOREIGN KEY (file_id) REFERENCES files (id)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tags (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS file_tags (
                file_id INTEGER,
                tag_id INTEGER,
                PRIMARY KEY (file_id, tag_id),
                FOREIGN KEY (file_id) REFERENCES files (id),
                FOREIGN KEY (tag_id) REFERENCES tags (id)
            )
        ''')
        
        # Create indexes
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_files_path ON files (file_path)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_files_type ON files (file_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_audio_artist ON audio_metadata (artist)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_audio_album ON audio_metadata (album)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_audio_genre ON audio_metadata (genre)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processing_type ON processing_metadata (process_type)')
        
        conn.commit()
        conn.close()
        
    def add_file(self, file_path, file_type, metadata=None):
        """Add a file to the database"""
        if not os.path.exists(file_path):
            return None
            
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Check if file already exists
            cursor.execute('SELECT id FROM files WHERE file_path = ?', (file_path,))
            existing = cursor.fetchone()
            
            if existing:
                return existing[0]
                
            # Add basic file info
            filename = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            file_format = os.path.splitext(file_path)[1][1:]
            created_date = datetime.fromtimestamp(os.path.getctime(file_path)).isoformat()
            modified_date = datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
            indexed_date = datetime.now().isoformat()
            
            cursor.execute('''
                INSERT INTO files (filename, file_path, file_type, file_size, format, created_date, modified_date, indexed_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (filename, file_path, file_type, file_size, file_format, created_date, modified_date, indexed_date))
            
            file_id = cursor.lastrowid
            
            # Add metadata if provided
            if metadata:
                self._add_metadata(cursor, file_id, metadata)
                
            conn.commit()
            return file_id
            
        except Exception as e:
            conn.rollback()
            logging.error(f"Error adding file to database: {e}")
            return None
            
        finally:
            conn.close()
            
    def _add_metadata(self, cursor, file_id, metadata):
        """Add metadata for a file"""
        # Add audio metadata if available
        audio_fields = [
            'duration', 'sample_rate', 'channels', 'bit_depth', 'bitrate',
            'title', 'artist', 'album', 'genre', 'date', 'track', 'comment'
        ]
        
        if any(field in metadata for field in audio_fields):
            values = [file_id]
            for field in audio_fields:
                values.append(metadata.get(field))
                
            placeholders = ', '.join(['?'] * (len(audio_fields) + 1))
            fields = 'file_id, ' + ', '.join(audio_fields)
            
            cursor.execute(f'''
                INSERT INTO audio_metadata ({fields})
                VALUES ({placeholders})
            ''', values)
            
        # Add processing metadata if available
        if 'process_type' in metadata:
            original_file = metadata.get('original_file')
            original_file_id = None
            
            if original_file:
                cursor.execute('SELECT id FROM files WHERE filename = ?', (original_file,))
                result = cursor.fetchone()
                if result:
                    original_file_id = result[0]
                    
            parameters = json.dumps(metadata.get('parameters', {}))
            
            cursor.execute('''
                INSERT INTO processing_metadata (file_id, original_file_id, process_type, processing_date, parameters)
                VALUES (?, ?, ?, ?, ?)
            ''', (file_id, original_file_id, metadata['process_type'], metadata.get('processing_date'), parameters))
            
        # Add tags if available
        if 'tags' in metadata and isinstance(metadata['tags'], list):
            for tag in metadata['tags']:
                tag_id = self._get_or_create_tag(cursor, tag)
                cursor.execute('''
                    INSERT OR IGNORE INTO file_tags (file_id, tag_id)
                    VALUES (?, ?)
                ''', (file_id, tag_id))
                
    def _get_or_create_tag(self, cursor, tag_name):
        """Get a tag ID or create if it doesn't exist"""
        cursor.execute('SELECT id FROM tags WHERE name = ?', (tag_name,))
        result = cursor.fetchone()
        
        if result:
            return result[0]
            
        cursor.execute('INSERT INTO tags (name) VALUES (?)', (tag_name,))
        return cursor.lastrowid
```

## Implementing Search and Data Deduplication

### 1. Create a File Search System

Implement a system to search for files:

```python
# data/file_search.py

class FileSearch:
    def __init__(self):
        self.metadata_db = MetadataDatabase()
        
    def search_files(self, query=None, file_type=None, audio_params=None, tags=None, date_range=None):
        """Search for files with specified criteria"""
        conn = sqlite3.connect(self.metadata_db.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Build query
        sql_parts = ["SELECT f.* FROM files f"]
        params = []
        
        # Join tables if needed
        if audio_params or any(key in ['title', 'artist', 'album', 'genre'] for key in (query or {})):
            sql_parts.append("LEFT JOIN audio_metadata am ON f.id = am.file_id")
            
        if tags:
            sql_parts.append("""
                JOIN file_tags ft ON f.id = ft.file_id
                JOIN tags t ON ft.tag_id = t.id
            """)
            
        # Where conditions
        where_parts = []
        
        if query:
            text_search = f"%{query}%"
            where_parts.append("""
                (f.filename LIKE ? OR am.title LIKE ? OR am.artist LIKE ? 
                OR am.album LIKE ? OR am.genre LIKE ?)
            """)
            params.extend([text_search] * 5)
            
        if file_type:
            where_parts.append("f.file_type = ?")
            params.append(file_type)
            
        if audio_params:
            for param, value in audio_params.items():
                if param in ['duration', 'sample_rate', 'channels', 'bit_depth', 'bitrate']:
                    # Handle ranges
                    if isinstance(value, dict):
                        if 'min' in value:
                            where_parts.append(f"am.{param} >= ?")
                            params.append(value['min'])
                        if 'max' in value:
                            where_parts.append(f"am.{param} <= ?")
                            params.append(value['max'])
                    else:
                        where_parts.append(f"am.{param} = ?")
                        params.append(value)
                        
        if tags:
            placeholders = ', '.join(['?'] * len(tags))
            where_parts.append(f"t.name IN ({placeholders})")
            params.extend(tags)
            
        if date_range:
            if 'start' in date_range:
                where_parts.append("f.created_date >= ?")
                params.append(date_range['start'])
            if 'end' in date_range:
                where_parts.append("f.created_date <= ?")
                params.append(date_range['end'])
                
        # Combine where conditions
        if where_parts:
            sql_parts.append("WHERE " + " AND ".join(where_parts))
            
        # Execute query
        sql = " ".join(sql_parts)
        cursor.execute(sql, params)
        
        # Fetch results
        results = []
        for row in cursor.fetchall():
            results.append(dict(row))
            
        conn.close()
        return results
```

### 2. Implement Data Deduplication

Create a system to identify and manage duplicate files:

```python
# data/deduplicator.py

class Deduplicator:
    def __init__(self):
        self.metadata_db = MetadataDatabase()
        
    def find_duplicates(self, criteria='hash'):
        """Find duplicate files"""
        if criteria == 'hash':
            return self._find_duplicates_by_hash()
        elif criteria == 'metadata':
            return self._find_duplicates_by_metadata()
        else:
            raise ValueError(f"Unknown duplicate criteria: {criteria}")
            
    def _find_duplicates_by_hash(self):
        """Find duplicates by file hash"""
        conn = sqlite3.connect(self.metadata_db.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get all files
        cursor.execute("SELECT id, file_path FROM files")
        all_files = cursor.fetchall()
        
        # Calculate hashes
        file_hashes = {}
        
        for file_id, file_path in [(f['id'], f['file_path']) for f in all_files]:
            if os.path.exists(file_path):
                file_hash = self._calculate_file_hash(file_path)
                
                if file_hash in file_hashes:
                    file_hashes[file_hash].append(file_id)
                else:
                    file_hashes[file_hash] = [file_id]
                    
        # Find duplicates
        duplicates = {}
        
        for file_hash, file_ids in file_hashes.items():
            if len(file_ids) > 1:
                # Get file details
                placeholders = ', '.join(['?'] * len(file_ids))
                cursor.execute(f"""
                    SELECT f.id, f.filename, f.file_path, f.file_type, f.file_size, 
                    f.created_date, f.modified_date
                    FROM files f
                    WHERE f.id IN ({placeholders})
                """, file_ids)
                
                duplicates[file_hash] = [dict(row) for row in cursor.fetchall()]
                
        conn.close()
        return duplicates
        
    def _calculate_file_hash(self, file_path, algorithm='sha256', block_size=65536):
        """Calculate file hash"""
        if not os.path.exists(file_path):
            return None
            
        hash_obj = hashlib.new(algorithm)
        
        with open(file_path, 'rb') as f:
            for block in iter(lambda: f.read(block_size), b''):
                hash_obj.update(block)
                
        return hash_obj.hexdigest()
        
    def _find_duplicates_by_metadata(self):
        """Find potential duplicates by audio metadata"""
        conn = sqlite3.connect(self.metadata_db.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Find files with same duration, title, artist and album
        cursor.execute("""
            SELECT am.duration, am.title, am.artist, am.album, COUNT(*) as count
            FROM audio_metadata am
            WHERE am.title IS NOT NULL AND am.artist IS NOT NULL
            GROUP BY am.duration, am.title, am.artist, am.album
            HAVING count > 1
        """)
        
        potential_dupes = cursor.fetchall()
        duplicates = {}
        
        for dupe in potential_dupes:
            cursor.execute("""
                SELECT f.id, f.filename, f.file_path, f.file_type, f.file_size, 
                f.created_date, f.modified_date
                FROM files f
                JOIN audio_metadata am ON f.id = am.file_id
                WHERE am.duration = ? AND am.title = ? AND am.artist = ? AND am.album = ?
            """, (dupe['duration'], dupe['title'], dupe['artist'], dupe['album']))
            
            key = f"{dupe['artist']}_{dupe['title']}_{dupe['album']}"
            duplicates[key] = [dict(row) for row in cursor.fetchall()]
            
        conn.close()
        return duplicates
```

## Conclusion

By implementing this data management strategy, you've created a robust system for organizing, tracking, and maintaining your audio files and processing results. This infrastructure will help you efficiently locate past work, minimize duplication, and ensure your valuable data remains accessible while keeping storage usage optimized.

### Next Steps

1. **Migrate existing files**: Organize your current audio files according to the new structure
2. **Run deduplication**: Identify and resolve duplicate files in your collection
3. **Create base metadata**: Generate metadata for your most important files
4. **Set up automated cleanup**: Schedule regular execution of retention policies
5. **Document your file organization**: Add this to your personal documentation

Proceed to [Security Considerations](11_security_considerations.md) to implement appropriate security measures for your data. 